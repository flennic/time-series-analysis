---
title: "Time Series Analysis - Teaching Session 01"
author: "Maximilian Pfundstein (maxpf364)"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{xcolor}
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
  html_document:
    df_print: paged
    toc: true
    toc_float: false
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 12

**Exercise:** Let ${x_t}$ be a zero-mean, unit-variance stationary process with autocorrelation function $\rho_h$. Suppose that $\mu_t$ is a nonconstant function and that $\sigma_t$ is a positive-valued nonconstant function. The observed series is formed as $y_t = \mu_t + \sigma_t x_t$.

**a)** Find the mean and covariance function for the ${y_t}$ process.

**b)** Show that the autocorrelation function for the ${y_t}$ process depends only on the time lag. Is the ${y_t}$ process stationary?

**c)** Is it possible to have a time series with a constant mean and with $\text{corr}(y_t, y_{t+h})$ free of $t$ but with ${y_t}$ *not* stationary?

## Mean and Covariance Function

### Mean

$$E[y_t] = E[\mu_t] + E[\sigma_tx_t] = E[\mu_t] + \sigma_t \color{red}{E[x_t]}$$

As $\color{red}{E[_t]}$ is $0$, we obtain

$$= E[\mu_t] = \mu_t$$

As $\mu_t$ is a non-constant function by definition.

### Covariance Function

$$\gamma(s,t) = \text{cov}(y_s, y_t) = E[(y_s - E[y_s])(y_t - E[y_t])]$$

We substitude $y_s = \mu_t + \sigma_t x_t$ (by definition) and we already know that $E[y_t] = \mu_t$.

$$= E[(\mu_s + \sigma_s x_s - \mu_s)(\mu_t + \sigma_t x_t - \mu_t)] = E[\sigma_s x_s \sigma_t x_t]$$

As $\sigma$ is a function by definition, there is no randomness and we can put it out of the expectation.

$$= \sigma_s \sigma_t E[ x_s x_t]$$

We know that $E[x_t] = 0$, so we can easily add it to get a known shape.

$$= \sigma_s \sigma_t E[ (x_s - E[x_s]) (x_t - E[x_t])]$$

The known shape is the definition of the covariance, so we obtain the following.

$$= \sigma_s \sigma_t \text{cov}(x_s, x_t) = \sigma_s \sigma_t \gamma(x_s, x_t)$$

The ACF is defined by

$$\rho(s,t) = \frac{\gamma(x_s, x_t)}{\sqrt{\gamma(x_s, x_s)\gamma(x_t, x_t)}}$$

Solving for $\gamma(s,t)$ and then plugging in in the equation above we obtain

$$= \sigma_s \sigma_t \rho(x_s, x_t) \sqrt{\gamma(x_s, x_s)\gamma(x_t, x_t)}$$

We know that $\gamma(x_s,x_s) = \text{var}(x_s) = 1$ as that is given by definition. Therefore the ACF is finally given by

$$= \sigma_s \sigma_t \rho(x_s, x_t) = \sigma_s \sigma_t \rho_x(s, t)$$

## Lag only Dependency and Stationarity of $y_t$

### Lag only Dependency

The autocorrelation is given by

$$\rho(s,t) = \frac{\gamma(s, t)}{\sqrt{\gamma(s, s)\gamma(t, t)}}$$

Plugging in the ACF from the exercise before gives us

$$=\frac{\sigma_s \sigma_t \rho_x(s,t)}{\sqrt{\sigma_s \sigma_s \rho_x(s,s) \sigma_t \sigma_t \rho_x(t,t)}}$$

As $\rho_x(a, a) = \frac{\gamma(0)}{\gamma(0)} = 1$ we can rewrite as

$$= \frac{\sigma_s \sigma_t \rho_x(s,t)}{\sigma_s \sigma_t} = \rho_x(s,t)$$

Setting $t = s + h$ we get the following, as the process $x_t$ is stationary by definition

$$\rho_x(s,s + h) = \rho(h)$$

### Stationarity of $y_t$

To show that $y_t$ is stationary, the following three conditions must be met:

- $E[y_t] = \text{constant}$
- $\gamma(s,t) = \gamma(|t + h - t|)$
- $\text{var}(x_t) < \infty$

The first condition if not given, as $E[y_t] = \mu_t$ and by definition $\mu_t$ is a non-constant function. Thereby $y_t$ is **not** stationary.

## Possibility of a Specific Time Series

For having a non-stationary time series that meets the condition, at least one of the following conditions must be violated.

- $E[y_t] = \text{constant}$
- $\gamma(s,t) = \gamma(|t + h - t|)$
- $\text{var}(x_t) < \infty$

We know by definition that the mean is constant and thus the first condition is *true*.

We also know that $\text{corr}(y_t, y_{t+h})$ is free of $t$, so

$$\text{corr}(y_t, y_{t+h}) = \frac{\text{cov}(y_t, y_{t+h})}{\sigma_{y_t} \sigma_{y_t + h}}$$

is free of $t$ as well. Therefore we know that $\text{cov}(y_t, y_{t+h}) = \gamma(y_t, y_{t+h}) = \gamma(h)$ as it will only depend on the lag $h$. So also the second condition is *true*.

As we don't have more information about the process, we could define a process that violates the last condition, which means that $\text{var}(y_t) = \infty$.

Therefore the answer is: **Yes**, it is possible to define a time series as requested in the question.

# Assignment 18

**Exercise:** For each of the following ARMA models, find the roots of the AR and MA polynomials, identify the values of p and q for which they are ARMA(p,q) (be careful of parameter redundancy), determine whether they are causal, and determine whether they are invertible. In each case, $w_t \sim \mathcal{N}(0,1)$.

**c)** $x_t - 3 x_{t-1} = w_t + 2 w_{t-1} - 8 w_{t-2}$

**d)** $x_t - 2 x_{t-1} + 2 x_{t-2} = w_t - \frac{8}{9} w_{t-1}$

**e)** $x_t - 4 x_{t-2} = w_t - w_{t-1} + 0.5 w_{t-2}$

**f)** $x_t - \frac{9}{4} x_{t-1} - \frac{9}{4} x_{t-2}=  w_t$


## c) **Solution**

### Roots of AR and MA Polynomials

Applying the autoregressive operator for $\phi$ and $\theta$ gives us:

AR: $\phi(B) = 1 - 3B$, thus $Z_{\phi} = (1, -3)$

MA: $\theta(B) = 1 + 2B -8B^2$, thus $Z_{\theta} = (1, 2, -8)$

For both parts we calculate the zero points. We start with AR.

$$p_\phi(z) = 1-3z$$

setting $p_\phi(z) = 0$

$$1-3z = 0$$

From that we get $z_{\theta1} = \frac{1}{3}$. Now we do the MA part.

$$p_\theta(z) = 1 + 2z -8z^2$$

setting $p_\theta(z) = 0$

$$0 = 1 + 2z -8z^2$$

First we divide by $-8$ and swap sides:

$$z^2 - \frac{1}{4}z - \frac{1}{8} = 0$$

We add $(\frac{1}{4}/2)^2$ on both sides

$$z^2 - \frac{1}{4}z - \frac{1}{8} + \frac{1}{64} - \frac{8}{64} = \frac{1}{64}$$

Next we create the polynomial form and add $\frac{8}{64}$ on both sides

$$(z-\frac{1}{8})^2 = \frac{9}{64}$$

Taking the square root

$$z-\frac{1}{8}2 = \pm \frac{3}{8}$$

Thus it follows that $z_{\theta1 = \frac{1}{2}}$ and $z_{\theta2 = - \frac{1}{4}}$.

### Finding p and q

To check for redundancy, we have to identify any common roots. As $z_\phi \cap z_\theta = \{\}$, $\phi(B)$ and $\theta(B)$ share no common roots and no redundancy is given.

Therefore p and q are simply given by the highest order terms, so the model is $\text{ARMA}(p = 1, q = 2)$.

### Causality and Invertibility

**Invertibility:** As $\forall Z_\theta: |Z_\theta| > 1$ is **not** given, the model is not causal.

**Causality:** As $\forall Z_\phi: |Z_\phi| > 1$ is **not** given, the model is not invertible.


## d) **Solution**

### Roots of AR and MA Polynomials

Applying the autoregressive operator for $\phi$ and $\theta$ gives us:

AR: $\phi(B) = 1 - 2 + 2B^2$, thus $Z_{\phi} = (1, -2, 2)$

MA: $\theta(B) = 1 - \frac{8}{9}B$, thus $Z_{\theta} = (1, -\frac{8}{9})$

For both parts we calculate the zero points. We start with AR.

$$p_\phi(z) = 1 - 2z + 2z^2$$

Setting $p_\phi(z) = 0$ and swapping sides

$$1 - 2z + 2z^2 = 0$$

Divide by 2 and order

$$z^2 - z + \frac{1}{2} = 0$$

We add $(1/2)^2$ on both sides

$$z^2 - z + \frac{1}{2} + \frac{1}{4} = \frac{1}{4}$$

Next we create the polynomial form and substract $\frac{1}{2}$ on both sides:

$$(z-\frac{1}{2})^2 = -\frac{1}{4}$$

Taking the square root:

$$z - \frac{1}{2} = \pm \frac{1}{2}i$$

From that we get $z_{\theta1} = \frac{1}{2} + \frac{1}{2}i$ and $z_{\theta2} = \frac{1}{2} - \frac{1}{2}i$. Now we do the MA part.

$$p_\theta(z) = 1 - \frac{8}{9}z$$

setting $p_\theta(z) = 0$

$$0 = 1 - \frac{8}{9}z$$

So $z_{\theta1 = \frac{9}{8}}$

### Finding p and q

To check for redundancy, we have to identify any common roots. As $z_\phi \cap z_\theta = \{\}$, $\phi(B)$ and $\theta(B)$ share no common roots and no redundancy is given.

Therefore p and q are simply given by the highest order terms, so the model is $\text{ARMA}(p = 2, q = 1)$.

### Causality and Invertibility

**Invertibility:** As $\forall Z_\theta: |Z_\theta| > 1$ is **not** given, the model is not causal.

**Causality:** As $\forall Z_\phi: |Z_\phi| > 1$ is given, the model is invertible. $|Z_\phi| = \frac{9}{8} = 1.125$.

## e) **Solution**

### Roots of AR and MA Polynomials

Applying the autoregressive operator for $\phi$ and $\theta$ gives us:

AR: $\phi(B) = 1 - 4B^2$, thus $Z_{\phi} = (1, 0, -4)$

MA: $\theta(B) = 1 - B + \frac{1}{2}B^2$, thus $Z_{\theta} = (1, -1, \frac{1}{2})$

For both parts we calculate the zero points. We start with AR.

$$p_\phi(z) = 1 - 4Z^2$$

setting $p_\phi(z) = 0$

$$1 - 4Z^2 = 0$$

dividing by $4$ and putting $Z$ on one side

$$Z^2 = \frac{1}{4}$$

From that we get $z_{\theta1} = \frac{1}{2}$ and $z_{\theta2} = -\frac{1}{2}$. Now we do the MA part.

$$p_\theta(z) = 1 - z + \frac{1}{2}z^2$$

setting $p_\theta(z) = 0$

$$0 = 1 - z + \frac{1}{2}z^2$$

First we divide by $\frac{1}{2}$ and swap sides:

$$z^2 -2z + 2 = 0$$

We add $(\frac{2}{2}/2)^2$ on both sides

$$z^2 -2z + 1 + 2 = 1$$

Next we create the polynomial form and substract $-2$ on both sides

$$(z-1)^2 = -1$$

Taking the square root

$$z - 1 = \pm i$$

Thus it follows that $z_{\theta1} = 1 + i$ and $z_{\theta2} = 1 - i$.

### Finding p and q

To check for redundancy, we have to identify any common roots. As $z_\phi \cap z_\theta = \{\}$, $\phi(B)$ and $\theta(B)$ share no common roots and no redundancy is given.

Therefore p and q are simply given by the highest order terms, so the model is $\text{ARMA}(p = 2, q = 2)$.

### Causality and Invertibility

**Invertibility:** As $\forall Z_\theta: |Z_\theta| > 1$ is **not** given, the model is not causal.

**Causality:** As $\forall Z_\phi: |Z_\phi| > 1$ is given, the model is invertible. The length of all elements of the set $Z_\phi$ is given by $|Z_\phi| = \sqrt{1^2 + (\pm 1)^2} \approx 1.414214$.

## f) **Solution**

### Roots of AR and MA Polynomials

Applying the autoregressive operator for $\phi$ and $\theta$ gives us:

AR: $\phi(B) = 1 - \frac{9}{4}B - \frac{9}{4}B^2$, thus $Z_{\phi} = (1, - \frac{9}{4}, - \frac{9}{4})$

MA: $\theta(B) = 1$, thus $Z_{\theta} = (1)$

For both parts we calculate the zero points. We start with AR.

$$p_\phi(z) = 1 - \frac{9}{4}z - \frac{9}{4} z^2$$

setting $p_\phi(z) = 0$

$$1 - \frac{9}{4}z - \frac{9}{4} z^2 = 0$$

dividing by $\frac{9}{4}$ and ordering

$$z^2 + z - \frac{4}{9} = 0$$

Adding $(\frac{1}{2})^2$ on both sides

$$z^2 + z + \frac{1}{4} - \frac{4}{9} = \frac{1}{4}$$

Forming the polynimial and adding $\frac{4}{9}$

$$(z + \frac{1}{2})^2 = \frac{25}{36}$$

Taking the square root

$$z + \frac{1}{2} = \pm \frac{5}{6}$$

From that we get $z_{\theta1} = \frac{1}{3}$ and $z_{\theta2} = -\frac{4}{3}$. Now we do the MA part.

$$p_\theta(z) = 1$$

As setting this to $0$ is an inequality, this ARMA model does not have an MA part.

### Finding p and q

To check for redundancy, we have to identify any common roots. As $\theta(B)$ is not given, it does not really make sense to ask this question. There are no common roots.

Therefore p and q are simply given by the highest order terms, so the model is $\text{ARMA}(p = 2, q = 0)$ which can also be expressed as $\text{AR}(2)$.

### Causality and Invertibility

**Invertibility:** As $\forall Z_\theta: |Z_\theta| > 1$ is **not** given, the model is not causal.

**Causality:** As the model does not have an MA part, it does not make sense to talk about invertibility as an $\text{MA}(\infty)$ representation cannot exist for something that is non-existent. Also a (weak) stationary property does not make sense in this case.