---
title: "Time Series Analysis - Lab 02 (Group 7)"
author: "Anubhav Dikshit (anudi287) and Maximilian Pfundstein (maxpf364)"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: false
    number_sections: false
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}
library(dplyr)
library(ggplot2)
library(kernlab)
library(astsa)
library(TSA)
knitr::opts_chunk$set(echo = TRUE)
set.seed(12345)
```

# Assignment 1: Computations with simulated data

## Linear Regressions on Necessarily Lagged Variables and Appropriate Correlation

**Task:** Generate $1000$ observations from AR(3) process with $\phi_1 = 0.8, \phi_2 = -0.2, \phi_3 = 0.1$. Use these data and the definition of PACF to compute $\phi_{33}$ from the sample, i.e. write your own code that performs linear regressions on necessarily lagged variables and then computes an appropriate correlation. Compare the result with the output of function `pacf()` and with the theoretical value of $\phi_{33}$.

**Answer:** First the sampling and looking at the built-in PACF.

```{r}

model = list(ar = c(0.8, -0.2, 0.1), ma = c())
set.seed(12345)
series = arima.sim(model = model, n = 1000)
pacf(series)
print(pacf(series))

```

Now we do it on our own.

```{r}

pacf_ar = function(series., lag.max = 30) {
  
  covariances = vector(length=lag.max)
  series = as.vector(series)
  
  for (lag in 1:lag.max) {
    
    # Create a dataframe with the lagged variables and omit NAs
    df = data.frame(y = series)
    df_colnames = c("y")
    
    if (lag == 1) {
      df = na.omit(cbind(df, lag(series, lag)))
      covariances[1] = cor(df[,1], df[,2])
      next
    }
    
    for (t in 1:(lag-1)) {
      df_colnames = c(df_colnames, paste("t_", t, sep=""))
      df = cbind(df, lag(series, t))
    }
    
    #df = na.omit(df)
    df = df[(1+lag):nrow(df),]
    colnames(df) = df_colnames
    
    # Second df
    df2 = data.frame(y = series)
    df2_colnames = c("y")
    
    for (t in 1:(lag-1)) {
      df2_colnames = c(df2_colnames, paste("t+", t, sep=""))
      df2 = cbind(df2, lead(series, t))
    }
    
    #df2 = na.omit(df2)
    df2 = df2[1:(nrow(df2)-lag),]
    colnames(df2) = df2_colnames
    
    # Performing LinReg
    x_t_dash  = lm(y ~ ., df)$residual
    x_t_dash_dash  = lm(y ~ ., df2)$residual
    
    covariances[lag] = cor(x_t_dash, x_t_dash_dash)
  }
  return(covariances)
}

pacf_ar(series, 3)
print(pacf(series, lag.max = 3))
ARMAacf(model$ar, lag.max = 3, pacf = TRUE)

```


## Methods of Moments, Conditional Least Squares and Maximum Likelihood

**Task:** Simulate an AR(2) series with $\phi_1 = 0.8, \phi_2 = 0.1$ and $n=100$. Compute the estimated parameters and their standand errors by using three methods: method of moments (Yule-Walker equations), conditional least squares and maximum likelihood (ML) and compare their results to the true values. Which method does seem to give the best result? Does theoretical value for $\phi_2$ fall within confidence interval for ML estimate?

**Answer:** Lets first simulate the time series and then fit the different models.

```{r, warning=FALSE}

model = list(ar = c(0.8, 0.1), ma = c())
set.seed(12345)
series = arima.sim(model = model, n = 100)

MOM_Model = ar(series, order = 2, method = "yule-walker", aic = FALSE)
CLS_Model = ar(series, order = 2, method = "ols", aic = FALSE)
ML_Model = ar(series, order = 2, method = "mle", aic = FALSE)


df = data.frame(MOM_Model$ar, CLS_Model$ar, ML_Model$ar)

df

```

It seems like the Methods of Moments works best in this case. Now lets look at the confidence interval.

As the function `ar()` does not seem to return the variance for the coefficients, we have to use the `arima()` function for that.

```{r, warning=FALSE}

ML_Model_CI = arima(series, order = c(2,0,0), method = "ML")

sigma = ML_Model_CI$var.coef[2, 2]
phi_2 = ML_Model_CI$coef[2]
CI = c(phi_2 - 1.96 * sigma, phi_2 + 1.96 * sigma)

CI

```

```{r, echo = FALSE}

is_within_ci = function() {
    if (df$ML_Model.ar[2] > CI[1] && df$ML_Model.ar[2] < CI[2]) {
    return("The phi_2 estimate lies within the confidence interval.")
  }
  return("The phi_2 estimate does not lie within the confidence interval.")
}

```

The $\phi_2$ estimate is `r df$ML_Model.ar[2]`, the CI is given by `r CI[1]` for the lower boundary and `r CI[2]` for the upper boundary. `r is_within_ci()`

## Sample and Theoretical ACF and PACF

**Task:** Generate $200$ observations of a seasonal $\text{ARIMA}(0,0,1) \times (0,0,1)_{12}$ model with coefficients $\Theta = 0.6$ and $\theta = 0.3$ by using `arima.sim()`. Plot sample ACF and PACF and also theoretical ACF and PACF. Which patterns can you see at the theoretical ACF and PACF? Are they repeated at the sample ACF and PACF?

**Answer:** TODO. Plotting of theoretical values and comparison.

```{r}

theta = 0.3
Theta = 0.6

model = list(ma = c(theta, rep(0, 10), Theta, theta*Theta))

series = arima.sim(model, n=200)

print(acf(series))
print(pacf(series))
ARMAacf(model$ma, lag.max = 3, pacf = TRUE)
ARMAacf(model$ma, lag.max = 3, pacf = FALSE)

```

## Forecast and Predition

**Task:** Generate $200$ observations of a seasonal $\text{ARIMA}(0,0,1) \times (0,0,1)_{12}$ model with coefficients $\Theta = 0.6$ and $\theta = 0.3$ by using `arima.sim()`. Fit $\text{ARIMA}(0,0,1) \times (0,0,1)_{12}$ model to the data, compute forecasts and a prediction band $30$ points ahead and plot the original data and the forecast with the prediction band. Fit the same data with function `gausspr()` from package `kernlab` (use default settings). Plot the original data and predicted data from $t = 1$ to $t = 230$. Compare the two plots and make conclusions.

```{r}

fitted_model = arima(series,
                     order = c(0, 0, 1),
                     seasonal = list(order= c(0, 0, 1), period = 12))

prediction = predict(fitted_model, n.ahead = 30)

```


```{r}

fitted_model_gausspr = gausspr(c(1:200), series)
prediction_gausspr = predict(fitted_model_gausspr, c(1:230))

```

```{r, echo=FALSE}

df = data.frame(time = c(1:length(series)),
                data = series)

df2 = data.frame(time = c((length(series)+1):
                            (length(prediction$pred)+length(series))),
                 forecast = prediction$pred,
                 upper_boundary = prediction$pred + 1.96*prediction$se,
                 lower_boundary = prediction$pred - 1.96*prediction$se)

df3 = data.frame(time = c(1:230),
                 gaussian = prediction_gausspr)

ggplot() +
  geom_ribbon(aes(x = df2$time,
                  ymin=df2$lower_boundary,
                  ymax=df2$upper_boundary),
                  fill = "#0000ff", alpha = 0.15) + 
  geom_line(aes(x = df$time, y = df$data, colour = "Original Series")) +
  geom_line(aes(x = df2$time, y = df2$forecast, colour = "Forecasted Series")) +
  geom_line(aes(x = df3$time, y = df3$gaussian, colour = "Forecasted Gaussian Series")) +
  labs(title = "LRandom Time Series", y = "Values", x = "Time", color = "Legend") +
  scale_color_manual(values = c("#FF5733", "#900C3F", "#444444")) +
  theme_minimal()


```

## Prediction Band

**Task:** Generate $50$ observations from ARMA(1, 1) process with $\phi = 0.7$, $\theta = 0.50$. Use first $40$ values to fit an ARMA(1,1) model with $\mu = 0$. Plot the data, the $95\%$ prediction band and plot also the true $10$ values that you initially dropped. How many of them are outside the prediction band? How can this be interpreted?

```{r}

model = list(ma = c(0.7), ar = c(0.5))
set.seed(12345)
series = arima.sim(model, n=50)
fitted_model = arima(series[1:40], order = c(1, 0, 1), include.mean = FALSE)
prediction = predict(fitted_model, n.ahead = 10)

```

```{r, echo=FALSE}

df = data.frame(time = c(1:length(series)),
                data = series)

df2 = data.frame(time = c(41:50),
                 forecast = prediction$pred,
                 upper_boundary = prediction$pred + 1.96*prediction$se,
                 lower_boundary = prediction$pred - 1.96*prediction$se)

ggplot() +
  geom_ribbon(aes(x = df2$time,
                  ymin=df2$lower_boundary,
                  ymax=df2$upper_boundary),
                  fill = "#0000ff", alpha = 0.15) + 
  geom_line(aes(x = df$time, y = df$data, colour = "Original Series")) +
  geom_line(aes(x = df2$time, y = df2$forecast, colour = "Forecasted Series")) +
  labs(title = "LRandom Time Series", y = "Values", x = "Time", color = "Legend") +
  scale_color_manual(values = c("#FF5733", "#900C3F")) +
  theme_minimal()

```


# Assignment 2: ACF and PACF diagnostics

## ARIMA Model Suggestion

**Task:** For data series `chicken` in package `astsa` (denote it by `x_t`) plot $4$ following graphs up to $40$ lags: ACF($x_t$), PACF($x_t$), ACF($\nabla x_t$), PACF($\nabla x_t$) (group them in one graph). Which ARIMA(p, d, q) or $\text{ARIMA}(p,d,q) \times (P,D,Q)_{s}$ models can be suggested based on this information only? Motivate your choice.

**Answer:** We will use this small helper function to create plots of teh time series, inclduing ACF and PACF, also for the first difference of the data.

```{r}

plot_diagnostics = function(series, max.lag = 40) {
  par(mfrow = c(3, 2))
  plot(series)
  plot(diff(series))
  acf(series, lag.max = max.lag)
  acf(diff(series), lag.max = max.lag)
  pacf(series, lag.max = max.lag)
  pacf(diff(series), lag.max = max.lag)
}

```

**Answer:** Looking at the ACF plot we see a decreasing correlation over time. As the correlation is continuingly following a downwards trend, while all of the lags keep being statistically significant, we should have a closer look at the first difference (this is because the original series in not stationary). Here we see a spike at lag 1 and 2, as well as an indicator for the seasionality with a lag of 12. Looking at the PACF plots we can observe the seasionality again, also we still see a significant spike at lag 1 and 2. Therefore we choose the following model: $\text{ARIMA}(2, 1, 0)\times(1, 0, 0)_{12}$.

```{r}

plot_diagnostics(astsa::chicken)

```

## More Datasets

**Task:** Repeat step 1 for the following datasets: `so2`, `EQcount`, `HCT` in package `astsa`.

**so2:** We see again that is makes sense to take the first difference to make the process stationary, as the ACF plot only shows a realyl slow decay. For the differenced ACF plot, we see three up two four significant spikes, but only the first one seems to be strongly significant. The differentiated PACF plot also shows several spikes in the beginning. It does not seem that there is a seasionality, so the chosen model is $\text{ARIMA}(0, 1, 1)$.

```{r}

plot_diagnostics(astsa::so2)

```

**EQcount:** We can see the typical behaviour of an AR process in the ACF plot, which is quickly decaying and we have no further increase in spikes. So we assume an underlying AR process. As we observe no seasionality, nor a major change in the differentiated plots, we chose the model $\text{AR}(1)$.

```{r}

plot_diagnostics(astsa::EQcount)

```

**HCT:** The ACF plot of the first difference shows a slowly decaying difference, while having recurring spikes. As the spikes are separated by 7 lags, we assume a weekly seasionality. The PACF shows significance at lag 3, as well as a significane for lag 7. Therefore we suggest the model $\text{ARIMA}(7, 1, 1)\times(0,0,1)_7$.

```{r}

plot_diagnostics(astsa::HCT)

```


# Assignment 3: ARIMA modeling cycle

In this assignment, you are assumed to apply a complete ARIMA modeling cycle starting from visualization and detrending and ending up with a forecasting.

## Finding a Suitable ARIMA Model (oil)

**Task:** Find a suitable ARIMA(p, d, q) model for the data set `oil` present in the library `astsa`. Your modeling should include the following steps in an appropriate order: visualization, unit root test, detrending by differencing (if necessary), transformations (if necessary), ACF and PACF plots when needed, EACF analysis, Q-Q plots, Box-Ljung test, ARIMA fit analysis, control of the parameter redundancy in the fitted model. When performing these steps, always have 2 tentative models at hand and select one of them in the end. Validate your choice by AIC and BIC and write down the equation of the selected model. Finally, perform forecasting of the model $20$ observations ahead and provide a suitable plot showing the forecast and its uncertainty.

**Answer:** As a first step we plot the data and take a first look at it. We can see that the time series is not stationary, so we will have to take a look at the first difference to gain more insights. Also it looks like that the data and variance is growing exponentially, so we will log the data first (transformation), then taking the difference.

```{r}

plot(astsa::oil)

```

Looking at the first difference, we can now see that the time series seems to be stationary. We will take a look at the ACF and PACF plots to obtain more information about the correlcation.

```{r}

par(mfrow = c(1, 2))
plot(log(astsa::oil))
plot(diff(log(astsa::oil)))

```

```{r}

par(mfrow = c(1, 2))
acf(diff(log(astsa::oil)))
pacf(diff(log(astsa::oil)))

```

Now it is time to decide for two models which we will use. Therefore we will also consider information gained from the `eacf()` function.

```{r}

eacf(diff(log(astsa::oil)))

```

We see a formed triangle at $\text{ARIMA}(0,1,3)$. As we have two equivalent models with a higher order, namely $\text{ARIMA}(0,1,4)$ and $\text{AIRMA}(1,1,3)$, we will consider the following two models for the following analysis.

```{r}

# We will use sarima as it will directly create the necessary plots
modelA = sarima(log(astsa::oil), p=0, d=1, q=3)
modelB = sarima(log(astsa::oil), p=1, d=1, q=3)

```

The AIC and BIC for the models are the following:

```{r}

# Lower AIC/BIC is better
AIC(modelA$fit)
BIC(modelA$fit)

AIC(modelB$fit)
BIC(modelB$fit)

```

According to the AIC and BIC score, both models seem to be nearly equally good. Also looking at the Q-Q plots we see that they almost have the exact same behaviour, having a mostly straight line for the quantiles while falling of towards the tails. `modelA` seems slightly better, so we choose this one.

Therefore the model is given by:

**TODO:** How do check for redundancy? Write the model (after discussion, otherwise you have to do it twice!).

The forecast looks like this.

```{r}

# Do we have to take the log here?!
sarima.for(log(astsa::oil), 0, 1, 3, n.ahead = 20)

```


## Finding a Suitable ARIMA Model (unemp)

**Task:** Find a suitable $\text{ARIMA}(p,d,q) \times (P,D,Q)_{s}$ model for the data set `unemp` present in the library `astsa`. Your modeling should include the following steps in an appropriate order: visualization, detrending by differencing (if necessary), transformations (if necessary), ACF and PACF plots when needed, EACF analysis, Q-Q plots, Box-Ljung test, ARIMA fit analysis, control of the parameter redundancy in the fitted model. When performing these steps, always have 2 tentative models at hand and select one of them in the end. Validate your choice by AIC and BIC and write down the equation of the selected model (write in the backshift operator notation without expanding the brackets). Finally, perform forecasting of the model $20$ observations ahead and provide a suitable plot showing the forecast and its uncertainty.

**Answer:** Again we take the data and take a first look at it. We see that is has some interesting climbs and that the series in not stationary. Therefore the next logical step is to take the difference. As we know from the description that we are dealing with monthly data, we will take the first difference with a lag of 12.

```{r}

par(mfrow = c(1, 2))
plot(astsa::unemp)
plot(diff(astsa::unemp, lag=12))

```

That looks better now, but stell the variance seems to be a problem, therefor we will also log the data. We see that the result is way better, seems stationary and the variance does not escalate.

```{r}

plot(diff(log(astsa::unemp), lag=12))

```

As a next step, after applying the differences and transformations, we will look at the ACF and PACF plot. We see, that even after taking the difference with lag 12, we still have seasionality.

```{r}

acf(diff(log(astsa::unemp), lag=12), lag.max = 12 * 6)
pacf(diff(log(astsa::unemp), lag=12), lag.max = 12 * 6)

```

It's time to look at the `eacf()` output. We consider the following two models: $\text{ARIMA}(2,1,2)\times(0,0,1)_{12}$ and $\text{ARIMA}(2,1,3)\times(0,0,1)_{12}$

```{r}

eacf(diff(log(astsa::unemp), lag=12))

```

Let's fit the models.

```{r}

modelA = sarima(log(astsa::unemp), p=2, d=1, q=3, P=0, D=0, Q=1, S=12)
modelB = sarima(log(astsa::unemp), p=2, d=1, q=2, P=0, D=0, Q=1, S=12)

```

The AIC and BIC for the models are the following:

```{r}

# Lower AIC/BIC is better
AIC(modelA$fit)
BIC(modelA$fit)

AIC(modelB$fit)
BIC(modelB$fit)

```

According to the AIC and BIC score, both models seem to be nearly equally good. Also looking at the Q-Q plots we see that `modelB` has a mostly straight line for the quantiles while falling of towards the tails. It looks better compared to `modelA`. It seems that `modelB` is slightly better, so we choose this one.

Finally the model is given by:

**TODO:** How do check for redundancy? Write the model (after discussion, otherwise you have to do it twice!).

```{r}

# Do we have to take the log here?!
sarima.for(astsa::unemp, p=2, d=1, q=2, P=0, D=0, Q=1, S=12, n.ahead = 20)

```

# Source Code

```{r, ref.label=knitr::all_labels(), echo = TRUE, eval = FALSE, results = 'show'}

```
